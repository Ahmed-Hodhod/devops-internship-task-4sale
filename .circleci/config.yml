version: 2.1
orbs:
  aws-eks: circleci/aws-eks@2.2.0
  kubernetes: circleci/kubernetes@1.3

  aws-ecs: circleci/aws-ecs@3.2.0

commands:
  destroy-environment:
    description: Destroy back-end and front-end cloudformation stacks  and remove files from s3
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            echo "Destroying environment:${CIRCLE_WORKFLOW_ID:0:7}"
            aws cloudformation delete-stack --stack-name updapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}
            aws cloudformation delete-stack --stack-name updapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}
            aws s3 rm s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7} --recursive

  revert-migrations:
    description: Revert the last migration
    steps:
      - run:
          name: Revert migrations
          when: on_fail
          command: |
            SUCCESS=$(curl --insecure  https://kvdb.io/MCQEkp8qSiznXmqbZS9PXE/migration_$\{CIRCLE_WORKFLOW_ID:0:7\})
            # Logic for reverting the database state
            if [[ $SUCCESS == 1 ]];
            then
                cd ~/project/backend
                npm install
                npm run migration:revert
                echo reverted successfully
            fi
jobs:
  build-api:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: API build
          command: |
            cd ping-pong-app
            npm install
            npm start &
      - save_cache:
          paths: [ping-pong-app/node_modules]
          key: api-build

  build-and-push-image:
    docker:
      - image: circleci/node:16.3.0
    steps:
      - checkout
      - setup_remote_docker
      - run:
          name: Build and push Docker image
          command: |
            # Build the Docker image
            cd ping-pong-app

            docker build -t $DOCKERHUB_USERNAME/ping-pong-app .

            # Log in to Docker Hub

            docker login -u $DOCKERHUB_USERNAME	 -p $DOCKERHUB_PASSWORD
            echo logged

            # Push the Docker image to Docker Hub
            docker push $DOCKERHUB_USERNAME/ping-pong-app

  deploy-infrastructure:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Deploy the cluster infrastructure
          no_output_timeout: 50m
          command: |

            aws cloudformation deploy --template-file .circleci/amazon-eks-vpc-private-subnets.yaml \
              --stack-name eks-4sale --parameter-overrides EKSIAMRoleName=eks_role \
              EKSClusterName=cluster-4sale --capabilities CAPABILITY_NAMED_IAM
      - run:
          name: Deploy the worker nodes
          no_output_timeout: 50m
          command: |
            sg=`aws cloudformation describe-stacks --region us-east-1  --query "Stacks[?StackName=='eks-4sale'][].Outputs[?OutputKey=='SecurityGroups'].OutputValue"  --output text`
            subnets=`aws cloudformation describe-stacks --region us-east-1  --query "Stacks[?StackName=='eks-4sale'][].Outputs[?OutputKey=='SubnetIds'].OutputValue"  --output text`
            vpc=`aws cloudformation describe-stacks --region us-east-1  --query "Stacks[?StackName=='eks-4sale'][].Outputs[?OutputKey=='VpcId'].OutputValue"  --output text`

            aws cloudformation deploy --template-file .circleci/amazon-eks-nodegroup.yaml \
             --stack-name eks-ng --capabilities CAPABILITY_NAMED_IAM --parameter-overrides  ClusterName=cluster-4sale ClusterControlPlaneSecurityGroup="${sg}" KeyName=eks NodeGroupName=eks-ng Subnets="${subnets}" VpcId="${vpc}"

  
      #- destroy-environment

  create-deployment:
    executor: aws-eks/python3
    steps:
      - checkout
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: cluster-4sale
          install-kubectl: true
      - kubernetes/create-or-update-resource:
          get-rollout-status: true
          resource-file-path: eks/app-deployment.yml
          resource-name: deployment/nginx-deployment

  test-cluster:
    docker:
      - image: cimg/python:3.10
  
    steps:
      - checkout
      - kubernetes/install:
          kubectl-version: v1.22.0
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: cluster-4sale
    
      - run:
          command: |
            kubectl apply -f eks/app-deployment.yml
            kubectl get services
            kubectl get deployments 
           
          name: Test cluster

  configure-infrastructure:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - add_ssh_keys:
          fingerprints: ["a3:87:ad:19:96:5f:07:73:7a:bd:b8:00:38:e1:54:36"]

      - run:
          name: Install dependencies needed
          command: |
            apk add python curl
            curl "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" -o "awscli-bundle.zip"
            unzip awscli-bundle.zip
            ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
            apk add --update ansible
      - attach_workspace:
          at: ~/
      - run:
          name: Configure server
          command: |
            echo "Current Working Directory: "
            pwd 
            #cd .circleci/ansible

            #pip install paramiko
            ansible-playbook --user=ubuntu  -c paramiko  -vvv -i  ~/hosts.txt .circleci/ansible/configure-server.yml

            #ansible localhost -m shell -a pwd    # for test purposes
      - destroy-environment

  run-migrations:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install awcli for the destroy-environment operation
          command: |
            sudo apt-get update 
            sudo apt-get install -y awscli
      - run:
          name: Run migrations
          command: |
            cd backend
            npm install
            # Run and save the migration output
            npm run migrations > migrations_dump.txt
          no_output_timeout: 30m
      - run:
          name: Send migration results to kvdb
          command: |
            if grep -q "has been executed successfully." backend/migrations_dump.txt
            then
              curl https://kvdb.io/MCQEkp8qSiznXmqbZS9PXE/$\{CIRCLE_WORKFLOW_ID:0:7\}_migration  -d '1'
            fi
      - destroy-environment
      - revert-migrations

  # Deploy Backend, Front Start
  deploy-frontend:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Download Dependencies
          command: |
            curl -fsSL https://rpm.nodesource.com/setup_13.x | bash - 
            yum -y install tar npm nodejs gzip
      - attach_workspace:
          at: ~/
      - run:
          name: Get backend url
          command: |
            #rm -rf ~/.url
            #cd frontend
            export BACKEND_IP=$(cat ~/hosts.txt | grep -v "all" )
            export API_URL="http://${BACKEND_IP}:3030"
            echo API_URL="http://${BACKEND_IP}:3030" >> frontend/.env
            echo $API_URL > ~/.url
            echo $API_URL
      - run:
          name: Deploy frontend objects
          command: |
            cd frontend
            npm install --save-dev webpack
            npm run build
            tar -czvf artifact-"${CIRCLE_WORKFLOW_ID:0:7}".tar.gz dist > /dev/null  
            aws s3 cp dist s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7} --recursive
      - persist_to_workspace:
          root: ~/
          paths:
            - .url
      - destroy-environment
      - revert-migrations

  deploy-backend:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - add_ssh_keys:
          fingerprints: ["a3:87:ad:19:96:5f:07:73:7a:bd:b8:00:38:e1:54:36"]
      - restore_cache:
          keys: [backend-build]
      - attach_workspace:
          at: ~/
      - run:
          name: Install dependencies
          command: |
            apk add python curl
            curl "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" -o "awscli-bundle.zip"
            unzip awscli-bundle.zip
            ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
            apk add --update ansible npm nodejs
      - run:
          name: Configure server
          command: |
            cd backend
            npm install
            npm run build
            cd ..
            # Zip the directory
            tar -C backend -czvf artifact.tar.gz .
            #cd .circleci/ansible 
            ansible-playbook -i  ~/hosts.txt .circleci/ansible/deploy-backend.yml
            echo "Contents  of the hosts.txt file is -------"
            cat ~/hosts.txt
      - destroy-environment
      - revert-migrations
  # Deploy Backend, Front End

  # Smoke Test Start
  smoke-test:
    docker:
      - image: alpine:latest
    steps:
      - checkout
      - attach_workspace:
          at: ~/
      - run:
          name: Install dependencies
          command: |
            apk add --update --no-cache curl aws-cli  
            apk add --update  nodejs npm
      - run:
          name: Backend smoke test.
          command: |
            export API_URL="$(cat ~/.url)/api/status"
            echo $API_URL
            if curl $API_URL | grep "ok"
              then
                echo Success
                return 0
              else
                echo Failure
                return 1
              fi
      - run:
          name: Frontend smoke test.
          command: |
            URL="http://udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website-us-east-1.amazonaws.com/#/employees"            
            #URL="http://udapeople-ab630c0.s3-website-us-east-1.amazonaws.com/#/employees"            
            if curl  $URL | grep "Welcome"
              then
                return 0
              else
                return 1
              fi
      - destroy-environment
      - revert-migrations
  # Smoke Test End

  # Swtich to production Start
  cloudfront-update:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Download dependencies
          command: |
            curl -fsSL https://rpm.nodesource.com/setup_13.x | bash - 
            yum -y install tar npm nodejs gzip
      - run:
          name: Update cloudfront distribution
          command: |
            # Change the origin of the cloudfront distro to do the switch from green to blue 
            #Get the old workflow id 
            export oldFullID=$(aws cloudformation list-exports \
            --query "Exports[?Name==\`WorkflowID\`].Value" \
            --no-paginate --output text)

            echo $oldFullID | cut -d- -f 2-2 > ~/OldWorkflowID.txt

            aws cloudformation deploy \
            --template-file .circleci/files/cloudfront.yml \
            --stack-name InitialStack \
            --parameter-overrides WorkflowID="udapeople-${CIRCLE_WORKFLOW_ID:0:7}" \

            echo OldWorkflowID: "$(cat ~/OldWorkflowID.txt)"
            echo Current Workflow ID: "${CIRCLE_WORKFLOW_ID:0:7}"

      # workflowID is the id of the newly created bucket that will be switched to it
      - persist_to_workspace:
          root: ~/
          paths:
            - OldWorkflowID.txt
      - destroy-environment
      - revert-migrations

  # Cleanup Start
  cleanup:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - attach_workspace:
          at: ~/
      - run:
          name: Fetch OldWorkFlowID and remove stacks and resources
          command: |
            export OldWorkflowID=$(cat ~/OldWorkflowID.txt)
            echo OldWorkflowID: "${OldWorkflowID}"
            echo CurrentWorkflowID: "${CIRCLE_WORKFLOW_ID:0:7}"
            if [[ "${CIRCLE_WORKFLOW_ID:0:7}" != "${OldWorkflowID}" ]]
              then
                echo "----Delete Confirmed-------"
                aws s3 rb "s3://udapeople-${OldWorkflowID}" --force  2> error.txt 
                
                aws cloudformation delete-stack --stack-name "udapeople-backend-${OldWorkflowID}" 2> error.txt 
                aws cloudformation delete-stack --stack-name "udapeople-frontend-${OldWorkflowID}" 2> error.txt 
              else
                echo "---Cannot cleanup----"
              fi

# Cleanup End

#--stack-name "InitialStack-${OldWorkflowID}" \
#--tags project=udapeople
# Swtich to production End

workflows:
  default:
    jobs:
      #- build-api
      #- build-and-push-image:
      #     requires: [build-api]
      # - test-api:
      #     requires: [build-api]
      # - scan-api:
      #     requires: [build-api]

      # - build-frontend
      # - build-backend
      # - test-frontend:
      #     requires: [build-frontend]
      # - test-backend:
      #     requires: [build-backend]
      # - scan-backend:
      #     requires: [build-backend]
      # - scan-frontend:
      #     requires: [build-frontend]
      - deploy-infrastructure
        # requires: [test-frontend, test-backend, scan-frontend, scan-backend]
      #-   create-deployment
      - test-cluster
      #     filters:
      #       branches:
      #         only: [master]
      # - configure-infrastructure:
      #     requires: [deploy-infrastructure]
      #     filters:
      #       branches:
      #         only: [master]
      # - run-migrations:
      #     requires: [configure-infrastructure]
      #     filters:
      #       branches:
      #         only: [master]
      # - deploy-frontend:
      #     #requires: [run-migrations]
      #     requires: [configure-infrastructure]
      #     filters:
      #       branches:
      #         only: [master]
      # - deploy-backend:
      #     #requires: [run-migrations]
      #     requires: [configure-infrastructure]
      #     filters:
      #       branches:
      #         only: [master]
      # # #- smoke-test:
      # # # requires: [deploy-backend, deploy-frontend]
      # # #filters:
      # # #branches:
      # # #   only: [master]
      # - cloudfront-update:
      #     #requires: [smoke-test]
      #     requires: [deploy-backend, deploy-frontend]
      #     filters:
      #       branches:
      #         only: [master]
      # - cleanup:
      #     requires: [cloudfront-update]
      #     filters:
      #       branches:
      #         only: [master]
